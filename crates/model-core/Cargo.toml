[package]
name = "model-core"
version.workspace = true
edition.workspace = true
description = "Core ML inference package for branch name generation"

[dependencies]
# ML/AI inference stack (package-specific)
candle-core = { git = "https://github.com/huggingface/candle.git", rev = "b1dbce09cd4cdc5af8069575a06646f0a858de7b", default-features = false }
candle-nn = { git = "https://github.com/huggingface/candle.git", rev = "b1dbce09cd4cdc5af8069575a06646f0a858de7b", default-features = false }
candle-transformers = { git = "https://github.com/huggingface/candle.git", rev = "b1dbce09cd4cdc5af8069575a06646f0a858de7b", default-features = false }
tokenizers = { version = "0.22", default-features = false, features = ["onig"] }

# Core utilities (shared via workspace)
anyhow.workspace = true
serde.workspace = true
serde_json.workspace = true
tracing.workspace = true

[features]
# Platform-conditional defaults for optimal GPU acceleration
# macOS: Accelerate is available and will be enabled by src-tauri
# Linux/Windows: No acceleration by default, CUDA available as opt-in feature
default = []

# Individual acceleration features
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
accelerate = ["candle-core/accelerate", "candle-nn/accelerate", "candle-transformers/accelerate"]
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]

[dev-dependencies]
pretty_assertions = { workspace = true }
test-log = { workspace = true }
